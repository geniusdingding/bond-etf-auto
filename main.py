import os
import io
import re
import pandas as pd
from datetime import datetime
import requests
import json
import collections

# ================= é…ç½®åŒºåŸŸ =================
INPUT_DIR = "input"       # æ”¾å…¥æ¯å¤©ä¸‹è½½çš„ xls æ–‡ä»¶çš„ç›®å½•
OUTPUT_DIR = "output"     # ç»“æœä¿å­˜ç›®å½•
ETF_PATH = os.path.join("config", "ç§‘åˆ›å€ºåå•.xlsx")  # æ‚¨çš„åå•æ¨¡æ¿è·¯å¾„
OUTPUT_FILE = os.path.join(OUTPUT_DIR, "ç§‘åˆ›å€ºETF_ç´¯è®¡ç»“æœ.xlsx")
# è¯·æ›¿æ¢ä¸ºæ‚¨çš„çœŸå®é£ä¹¦ Webhook
WEBHOOK_URL = "https://open.feishu.cn/open-apis/bot/v2/hook/fc7e6de2-fa45-4c14-96ac-c7bda5874732"
# ===========================================


# âœ… è¯»å– push å¼€å…³
def load_push_config():
    cfg_path = "config.json"
    if not os.path.exists(cfg_path):
        print("âš ï¸ æœªæ‰¾åˆ° config.jsonï¼Œé»˜è®¤ push_enabled=False")
        return False

    try:
        with open(cfg_path, "r", encoding="utf-8") as f:
            cfg = json.load(f)
            enabled = cfg.get("push_enabled", False)
            print(f"ğŸš¦ æ¨é€å¼€å…³çŠ¶æ€: {enabled}")
            return enabled
    except:
        print("âš ï¸ config.json è§£æå¤±è´¥ï¼Œé»˜è®¤ push_enabled=False")
        return False


def extract_date_from_filename(filename: str) -> str:
    basename = os.path.basename(filename)
    m = re.search(r"(\d{8})", basename)
    if not m:
        return None
    date_str = m.group(1)
    return f"{date_str[0:4]}/{date_str[4:6]}/{date_str[6:8]}"


def group_files_by_date():
    files_map = collections.defaultdict(list)

    if not os.path.exists(INPUT_DIR):
        os.makedirs(INPUT_DIR)
        print(f"âš ï¸ ç›®å½• {INPUT_DIR} ä¸å­˜åœ¨ï¼Œå·²è‡ªåŠ¨åˆ›å»ºï¼Œè¯·æ”¾å…¥ xls æ–‡ä»¶ã€‚")
        return {}

    raw_files = [f for f in os.listdir(INPUT_DIR) if f.lower().endswith((".xls", ".xlsx", ".csv"))]

    if not raw_files:
        print("âš ï¸ input ç›®å½•æ²¡æœ‰ä»»ä½• Excel æ–‡ä»¶")
        return {}

    for f in raw_files:
        date_str = extract_date_from_filename(f)
        if date_str:
            full_path = os.path.join(INPUT_DIR, f)
            files_map[date_str].append(full_path)
        else:
            print(f"âš ï¸ è·³è¿‡æ— æ³•æå–æ—¥æœŸçš„æ–‡ä»¶: {f}")

    sorted_dates = sorted(files_map.keys())
    print(f"âœ… æ‰«æåˆ° {len(sorted_dates)} ä¸ªæ—¥æœŸçš„æ–‡ä»¶å¾…å¤„ç†")
    return {date: files_map[date] for date in sorted_dates}


def read_file_data(file_path):
    filename = os.path.basename(file_path)

    if "æ·±åœ³" in filename:
        header_row = 4
        print(f"   â†’ è¯»å–æ·±åœ³æ–‡ä»¶ (Header=5): {filename}")
    else:
        header_row = 2
        print(f"   â†’ è¯»å–ä¸Šæµ·æ–‡ä»¶ (Header=3): {filename}")

    with open(file_path, "rb") as f:
        file_stream = io.BytesIO(f.read())

    try:
        df = pd.read_excel(file_stream, header=header_row)
    except:
        file_stream.seek(0)
        try:
            df = pd.read_csv(file_stream, header=header_row, sep=None, engine="python", encoding='gbk')
        except:
            df = pd.read_csv(file_stream, header=header_row, sep=None, engine="python", encoding='utf-8')

    cols = df.columns.tolist()
    col_code = next((c for c in cols if 'ä»£ç ' in str(c)), cols[0])
    col_rate = next((c for c in cols if 'æŠ˜ç®—' in str(c)), cols[2] if len(cols) > 2 else cols[1])

    df = df.dropna(subset=[col_code])
    df[col_code] = pd.to_numeric(df[col_code], errors="coerce")
    df = df.dropna(subset=[col_code])
    df[col_code] = df[col_code].astype("Int64")

    df[col_rate] = pd.to_numeric(df[col_rate], errors="coerce")

    if "æ·±åœ³" in filename:
        print("     âš¡ï¸ æ·±åœ³æ•°æ®ä¿®æ­£ x100")
        df[col_rate] = df[col_rate] * 100

    df[col_rate] = df[col_rate].round(0).astype("Int64")
    return dict(zip(df[col_code], df[col_rate]))


def process_date_group(date_str, file_list, df_result):
    print(f"ğŸ“… å¼€å§‹å¤„ç†æ—¥æœŸ: {date_str}")
    combined_map = {}

    for file_path in file_list:
        try:
            file_map = read_file_data(file_path)
            combined_map.update(file_map)
        except Exception as e:
            print(f"âŒ è¯»å–å¤±è´¥ {os.path.basename(file_path)}: {e}")

    df_result["åŸºé‡‘ä»£ç "] = pd.to_numeric(df_result["åŸºé‡‘ä»£ç "], errors="coerce").astype("Int64")
    df_result[date_str] = df_result["åŸºé‡‘ä»£ç "].map(combined_map)
    return df_result


def sort_columns(df):
    fixed_cols = ["åŸºé‡‘ä»£ç ", "åŸºé‡‘ç®€ç§°"]
    date_cols = sorted([c for c in df.columns if c not in fixed_cols])
    return df[fixed_cols + date_cols]


def send_to_feishu(file_name, summary_text=None):
    raw_url = f"https://raw.githubusercontent.com/geniusdingding/bond-etf-auto/main/output/{file_name}"

    data = {
        "msg_type": "post",
        "content": {
            "post": {
                "zh_cn": {
                    "title": "ğŸ“Š ç§‘åˆ›å€ºæŠ˜ç®—ç‡è‡ªåŠ¨æ›´æ–°",
                    "content": [
                        [{"tag": "text", "text": summary_text or "âœ… æ•°æ®å·²æ›´æ–°"}],
                        [{"tag": "a", "text": "ğŸ“ ç‚¹å‡»ä¸‹è½½æœ€æ–°ç´¯è®¡è¡¨æ ¼", "href": raw_url}]
                    ]
                }
            }
        }
    }

    try:
        resp = requests.post(WEBHOOK_URL, data=json.dumps(data), headers={"Content-Type": "application/json"})
        print("âœ… é£ä¹¦æ¨é€ç»“æœ:", resp.text)
    except Exception as e:
        print("âŒ é£ä¹¦æ¨é€å¤±è´¥:", e)


# ================= MAIN =================
if __name__ == "__main__":

    push_enabled = load_push_config()

    if not os.path.exists(ETF_PATH):
        if os.path.exists("ç§‘åˆ›å€ºåå•.xlsx"):
            ETF_PATH = "ç§‘åˆ›å€ºåå•.xlsx"
        else:
            raise FileNotFoundError(f"âŒ æ‰¾ä¸åˆ°é…ç½®æ–‡ä»¶: {ETF_PATH}")

    df_template = pd.read_excel(ETF_PATH)[["åŸºé‡‘ä»£ç ", "åŸºé‡‘ç®€ç§°"]]

    if os.path.exists(OUTPUT_FILE):
        print(f"âœ… åŠ è½½å†å²æ–‡ä»¶: {OUTPUT_FILE}")
        df_result = pd.read_excel(OUTPUT_FILE)
    else:
        print("âœ… åˆå§‹åŒ–æ–°æ–‡ä»¶")
        df_result = df_template.copy()

    grouped_files = group_files_by_date()

    for date_str, files in grouped_files.items():
        df_result = process_date_group(date_str, files, df_result)

    df_result = sort_columns(df_result)

    os.makedirs(OUTPUT_DIR, exist_ok=True)
    df_result.to_excel(OUTPUT_FILE, index=False)
    print(f"ğŸ‰ ç´¯è®¡ç»“æœå·²ä¿å­˜: {OUTPUT_FILE}")

    cols = df_result.columns.tolist()
    date_cols = [c for c in cols if c not in ["åŸºé‡‘ä»£ç ", "åŸºé‡‘ç®€ç§°"]]

    if date_cols:
        latest_date = date_cols[-1]
        valid_data = df_result[latest_date].dropna()
        count = len(valid_data)
        avg_rate = round(valid_data.mean(), 2) if count > 0 else 0

        summary = (
            f"ğŸ“… æœ€æ–°æ•°æ®æ—¥æœŸ: {latest_date}\n"
            f"ğŸ“ˆ å¯è´¨æŠ¼ETFæ•°é‡: {count} åª\n"
            f"ğŸ’° å¹³å‡æŠ˜ç®—ç‡: {avg_rate}"
        )

        print(f"\næ‘˜è¦ä¿¡æ¯:\n{summary}\n")

        if push_enabled:
            send_to_feishu("ç§‘åˆ›å€ºETF_ç´¯è®¡ç»“æœ.xlsx", summary)
            print("ğŸš€ å·²æ‰§è¡Œé£ä¹¦æ¨é€")
        else:
            print("âœ… push_enabled=False â†’ è·³è¿‡é£ä¹¦æ¨é€")

    if os.getenv("GITHUB_ACTIONS") and push_enabled:
        print("ğŸ¤– GitHub Actions ç¯å¢ƒï¼Œæ‰§è¡Œè‡ªåŠ¨æäº¤")
        os.system('git config --local user.email "action@github.com"')
        os.system('git config --local user.name "GitHub Action"')
        os.system(f"git add {OUTPUT_FILE}")
        os.system('git commit -m "Auto update bond ETF rates" || echo "No changes"')
    else:
        print("ğŸ’» æœ¬åœ°è¿è¡Œæˆ–æ¨é€å…³é—­ â†’ è·³è¿‡è‡ªåŠ¨æäº¤")
