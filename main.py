import os
import io
import re
import pandas as pd
from datetime import datetime
import requests
import json
import collections

# ================= é…ç½®åŒºåŸŸ =================
INPUT_DIR = "input"       # æ”¾å…¥æ¯å¤©ä¸‹è½½çš„ xls æ–‡ä»¶çš„ç›®å½•
OUTPUT_DIR = "output"     # ç»“æœä¿å­˜ç›®å½•
ETF_PATH = os.path.join("config", "ç§‘åˆ›å€ºåå•.xlsx")  # æ‚¨çš„åå•æ¨¡æ¿è·¯å¾„
OUTPUT_FILE = os.path.join(OUTPUT_DIR, "ç§‘åˆ›å€ºETF_ç´¯è®¡ç»“æœ.xlsx")
# è¯·æ›¿æ¢ä¸ºæ‚¨çš„çœŸå®é£ä¹¦ Webhook
WEBHOOK_URL = "https://open.feishu.cn/open-apis/bot/v2/hook/fc7e6de2-fa45-4c14-96ac-c7bda5874732"
# ===========================================

def load_push_config():
    cfg_path = "config.json"
    if not os.path.exists(cfg_path):
        return False
    try:
        with open(cfg_path, "r", encoding="utf-8") as f:
            cfg = json.load(f)
            return cfg.get("push_enabled", False)
    except:
        return False

def extract_date_from_filename(filename: str) -> str:
    """
    ä»æ–‡ä»¶åä¸­æå–8ä½æ•°å­—æ—¥æœŸï¼Œä¾‹å¦‚ 20251124 -> 2025/11/24
    """
    basename = os.path.basename(filename)
    m = re.search(r"(\d{8})", basename)
    if not m:
        return None
    date_str = m.group(1)
    return f"{date_str[0:4]}/{date_str[4:6]}/{date_str[6:8]}"

def group_files_by_date():
    """
    æ‰«æ input ç›®å½•ï¼ŒæŒ‰æ—¥æœŸå°†æ–‡ä»¶åˆ†ç»„
    """
    files_map = collections.defaultdict(list)
    
    if not os.path.exists(INPUT_DIR):
        os.makedirs(INPUT_DIR)
        print(f"âš ï¸ ç›®å½• {INPUT_DIR} ä¸å­˜åœ¨ï¼Œå·²è‡ªåŠ¨åˆ›å»ºï¼Œè¯·æ”¾å…¥ xls æ–‡ä»¶ã€‚")
        return {}

    raw_files = [f for f in os.listdir(INPUT_DIR) if f.lower().endswith((".xls", ".xlsx", ".csv"))]
    
    if not raw_files:
        print("âš ï¸ input ç›®å½•æ²¡æœ‰ä»»ä½• Excel æ–‡ä»¶")
        return {}

    for f in raw_files:
        date_str = extract_date_from_filename(f)
        if date_str:
            full_path = os.path.join(INPUT_DIR, f)
            files_map[date_str].append(full_path)

    # æŒ‰æ—¥æœŸæ’åº
    sorted_dates = sorted(files_map.keys())
    print(f"âœ… æ‰«æåˆ° {len(sorted_dates)} ä¸ªæ—¥æœŸçš„æ–‡ä»¶å¾…å¤„ç†")
    
    return {date: files_map[date] for date in sorted_dates}

def read_file_data(file_path):
    """
    è¯»å–å•ä¸ªæ–‡ä»¶ï¼Œè¿”å› {ä»£ç : æŠ˜ç®—ç‡} çš„å­—å…¸
    è‡ªåŠ¨åˆ¤æ–­æ˜¯ä¸Šæµ·è¿˜æ˜¯æ·±åœ³æ ¼å¼ï¼Œå¹¶ç»Ÿä¸€å•ä½ä¸ºæ•´æ•°
    """
    filename = os.path.basename(file_path)
    
    if "æ·±åœ³" in filename:
        header_row = 4
        print(f"   â†’ è¯»å–æ·±åœ³æ–‡ä»¶ (Header=5): {filename}")
    else:
        header_row = 2
        print(f"   â†’ è¯»å–ä¸Šæµ·æ–‡ä»¶ (Header=3): {filename}")

    with open(file_path, "rb") as f:
        file_stream = io.BytesIO(f.read())

    try:
        df = pd.read_excel(file_stream, header=header_row)
    except:
        file_stream.seek(0)
        try:
            df = pd.read_csv(file_stream, header=header_row, sep=None, engine="python", encoding='gbk')
        except:
            df = pd.read_csv(file_stream, header=header_row, sep=None, engine="python", encoding='utf-8')

    cols = df.columns.tolist()
    col_code = next((c for c in cols if 'ä»£ç ' in str(c)), None)
    col_rate = next((c for c in cols if 'æŠ˜ç®—' in str(c)), None)

    if not col_code or not col_rate:
        col_code = cols[0]
        col_rate = cols[2] if len(cols) > 2 else cols[1]

    df = df.dropna(subset=[col_code])
    df[col_code] = pd.to_numeric(df[col_code], errors="coerce")
    df = df.dropna(subset=[col_code])
    df[col_code] = df[col_code].astype("Int64")
    df[col_rate] = pd.to_numeric(df[col_rate], errors="coerce")
    
    if "æ·±åœ³" in filename:
        print(f"     âš¡ï¸ æ£€æµ‹åˆ°æ·±åœ³æ•°æ®ï¼Œæ‰§è¡Œ x100 ä¿®æ­£")
        df[col_rate] = df[col_rate] * 100
    
    df[col_rate] = df[col_rate].round(0).astype("Int64")
    
    return dict(zip(df[col_code], df[col_rate]))

def process_date_group(date_str, file_list, df_result):
    print(f"ğŸ“… å¼€å§‹å¤„ç†æ—¥æœŸ: {date_str}")
    combined_map = {}
    
    for file_path in file_list:
        try:
            file_map = read_file_data(file_path)
            combined_map.update(file_map)
        except Exception as e:
            print(f"âŒ è¯»å–æ–‡ä»¶å¤±è´¥ {os.path.basename(file_path)}: {e}")

    df_result["åŸºé‡‘ä»£ç "] = pd.to_numeric(df_result["åŸºé‡‘ä»£ç "], errors="coerce").astype("Int64")
    df_result[date_str] = df_result["åŸºé‡‘ä»£ç "].map(combined_map)
    return df_result

def sort_columns(df):
    """
    åˆ—æ’åºï¼š
    1. å›ºå®šåˆ—åœ¨å·¦
    2. æ—¥æœŸåˆ—æŒ‰ã€ä»æ–°åˆ°æ—§ã€‘æ’åº (reverse=True)
    """
    fixed_cols = ["åŸºé‡‘ä»£ç ", "åŸºé‡‘ç®€ç§°"]
    # æ‰¾å‡ºæ—¥æœŸåˆ—ï¼Œå¹¶å€’åºæ’åˆ—ï¼ˆæœ€è¿‘çš„æ—¥æœŸåœ¨æœ€å·¦è¾¹ï¼‰
    date_cols = sorted([c for c in df.columns if c not in fixed_cols], reverse=True)
    return df[fixed_cols + date_cols]

def send_to_feishu(file_name, title_text, content_text):
    """
    å‘é€é£ä¹¦æ¶ˆæ¯ï¼š
    file_name: ä¸‹è½½çš„æ–‡ä»¶å
    title_text: æ¶ˆæ¯æ ‡é¢˜ï¼ˆåŒ…å«æ—¥æœŸï¼‰
    content_text: æ¶ˆæ¯æ­£æ–‡ï¼ˆåŒ…å«ç»Ÿè®¡å’Œæ˜ç»†ï¼‰
    """
    # æ„é€  GitHub Raw é“¾æ¥
    # æ³¨æ„ï¼šè¿™é‡Œå‡è®¾æ‚¨çš„åˆ†æ”¯åæ˜¯ auto-updates
    raw_url = f"https://raw.githubusercontent.com/geniusdingding/bond-etf-auto/auto-updates/output/{file_name}"
    
    data = {
        "msg_type": "post",
        "content": {
            "post": {
                "zh_cn": {
                    "title": title_text,
                    "content": [
                        [{"tag": "text", "text": content_text}],
                        [{"tag": "text", "text": "\n--------------------\n"}],
                        [{"tag": "a", "text": "ğŸ“ ç‚¹å‡»ä¸‹è½½æœ€æ–°ç´¯è®¡è¡¨æ ¼ (éœ€GitHubåŒæ­¥)", "href": raw_url}]
                    ]
                }
            }
        }
    }

    try:
        resp = requests.post(WEBHOOK_URL, data=json.dumps(data), headers={"Content-Type": "application/json"})
        print("âœ… é£ä¹¦æ¨é€ç»“æœ:", resp.text)
    except Exception as e:
        print("âŒ é£ä¹¦æ¨é€å¤±è´¥:", e)

if __name__ == "__main__":
    push_enabled = load_push_config()

    if not os.path.exists(ETF_PATH):
        if os.path.exists("ç§‘åˆ›å€ºåå•.xlsx"):
             ETF_PATH = "ç§‘åˆ›å€ºåå•.xlsx"
        else:
             raise FileNotFoundError(f"âŒ æ‰¾ä¸åˆ°é…ç½®æ–‡ä»¶: {ETF_PATH}")
    
    df_template = pd.read_excel(ETF_PATH)[["åŸºé‡‘ä»£ç ", "åŸºé‡‘ç®€ç§°"]]

    if os.path.exists(OUTPUT_FILE):
        print(f"âœ… åŠ è½½å†å²æ–‡ä»¶: {OUTPUT_FILE}")
        df_result = pd.read_excel(OUTPUT_FILE)
    else:
        print("âœ… åˆå§‹åŒ–æ–°æ–‡ä»¶")
        df_result = df_template.copy()

    grouped_files = group_files_by_date()
    
    if not grouped_files:
        print("âš ï¸ æ²¡æœ‰éœ€è¦å¤„ç†çš„æ–‡ä»¶ï¼Œè„šæœ¬ç»“æŸ")
    else:
        for date_str, files in grouped_files.items():
            df_result = process_date_group(date_str, files, df_result)

        # 1. æ’åºï¼šæœ€è¿‘çš„æ—¥æœŸåœ¨å·¦è¾¹
        df_result = sort_columns(df_result)
        
        # 2. ä¿å­˜ç»“æœ
        os.makedirs(OUTPUT_DIR, exist_ok=True)
        df_result.to_excel(OUTPUT_FILE, index=False)
        print(f"ğŸ‰ ç´¯è®¡ç»“æœå·²ä¿å­˜: {OUTPUT_FILE}")

        # 3. ç”Ÿæˆè¯¦ç»†æ‘˜è¦
        cols = df_result.columns.tolist()
        date_cols = [c for c in cols if c not in ["åŸºé‡‘ä»£ç ", "åŸºé‡‘ç®€ç§°"]]
        
        if date_cols:
            # å› ä¸ºæˆ‘ä»¬å·²ç» sort(reverse=True) äº†ï¼Œæ‰€ä»¥ç¬¬0ä¸ªå°±æ˜¯æœ€æ–°çš„æ—¥æœŸ
            latest_date = date_cols[0]
            
            # ç­›é€‰å‡ºæœ€æ–°è¿™ä¸€å¤©æœ‰æ•°æ®çš„æ‰€æœ‰è¡Œ
            # .dropna() è‡ªåŠ¨è¿‡æ»¤æ‰ NaN çš„è¡Œ
            day_data = df_result[['åŸºé‡‘ç®€ç§°', latest_date]].dropna()
            
            count = len(day_data)
            
            if count > 0:
                avg_rate = day_data[latest_date].mean()
                
                # æ„é€ æ ‡é¢˜
                msg_title = f"ğŸ“Š ç§‘åˆ›å€ºETFæŠ˜ç®—ç‡ ({latest_date})"
                
                # æ„é€ ç»Ÿè®¡ä¿¡æ¯
                msg_content = f"ğŸ“ˆ å‚ä¸è´¨æŠ¼ETF: {count} å®¶\nğŸ’° å¹³å‡æŠ˜ç®—ç‡: {round(avg_rate, 2)}\n\nğŸ“‹ å½“æ—¥æ˜ç»†:"
                
                # å¾ªç¯ç½—åˆ—æ‰€æœ‰æœ‰æ•°æ®çš„ ETF
                for _, row in day_data.iterrows():
                    name = row['åŸºé‡‘ç®€ç§°']
                    rate = int(row[latest_date]) # è½¬æ•´æ•°æ˜¾ç¤º
                    msg_content += f"\nâ€¢ {name}: {rate}"
            else:
                msg_title = f"ğŸ“Š ç§‘åˆ›å€ºETFæŠ˜ç®—ç‡ ({latest_date})"
                msg_content = "âš ï¸ å½“æ—¥æš‚æ— åŒ¹é…æ•°æ®"

            print(f"\næ‘˜è¦ä¿¡æ¯:\n{msg_title}\n{msg_content}\n")

            if push_enabled:
                # å‘é€é£ä¹¦
                send_to_feishu("ç§‘åˆ›å€ºETF_ç´¯è®¡ç»“æœ.xlsx", msg_title, msg_content)
                print("ğŸš€ å·²æ‰§è¡Œé£ä¹¦æ¨é€")
            else:
                print("âœ… push_enabled=False â†’ è·³è¿‡é£ä¹¦æ¨é€")