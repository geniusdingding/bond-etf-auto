import os
import io
import re
import pandas as pd
from datetime import datetime
import requests
import json
import collections

# ================= é…ç½®åŒºåŸŸ =================
INPUT_DIR = "input"       # æ”¾å…¥æ¯å¤©ä¸‹è½½çš„ xls æ–‡ä»¶çš„ç›®å½•
OUTPUT_DIR = "output"     # ç»“æœä¿å­˜ç›®å½•
ETF_PATH = os.path.join("config", "ç§‘åˆ›å€ºåå•.xlsx")  # æ‚¨çš„åå•æ¨¡æ¿è·¯å¾„
OUTPUT_FILE = os.path.join(OUTPUT_DIR, "ç§‘åˆ›å€ºETF_ç´¯è®¡ç»“æœ.xlsx")
# è¯·æ›¿æ¢ä¸ºæ‚¨çš„çœŸå®é£ä¹¦ Webhook
WEBHOOK_URL = "https://open.feishu.cn/open-apis/bot/v2/hook/fc7e6de2-fa45-4c14-96ac-c7bda5874732"
# ===========================================

def extract_date_from_filename(filename: str) -> str:
    """
    ä»æ–‡ä»¶åä¸­æå–8ä½æ•°å­—æ—¥æœŸï¼Œä¾‹å¦‚ 20251124 -> 2025/11/24
    """
    basename = os.path.basename(filename)
    m = re.search(r"(\d{8})", basename)
    if not m:
        return None
    date_str = m.group(1)
    return f"{date_str[0:4]}/{date_str[4:6]}/{date_str[6:8]}"

def group_files_by_date():
    """
    æ‰«æ input ç›®å½•ï¼ŒæŒ‰æ—¥æœŸå°†æ–‡ä»¶åˆ†ç»„
    è¿”å›æ ¼å¼: { '2025/11/26': ['input/ä¸Šæµ·...xls', 'input/æ·±åœ³...xls'], ... }
    """
    files_map = collections.defaultdict(list)
    
    if not os.path.exists(INPUT_DIR):
        os.makedirs(INPUT_DIR)
        print(f"âš ï¸ ç›®å½• {INPUT_DIR} ä¸å­˜åœ¨ï¼Œå·²è‡ªåŠ¨åˆ›å»ºï¼Œè¯·æ”¾å…¥ xls æ–‡ä»¶ã€‚")
        return {}

    raw_files = [f for f in os.listdir(INPUT_DIR) if f.lower().endswith((".xls", ".xlsx", ".csv"))]
    
    if not raw_files:
        print("âš ï¸ input ç›®å½•æ²¡æœ‰ä»»ä½• Excel æ–‡ä»¶")
        return {}

    for f in raw_files:
        date_str = extract_date_from_filename(f)
        if date_str:
            full_path = os.path.join(INPUT_DIR, f)
            files_map[date_str].append(full_path)
        else:
            print(f"âš ï¸ è·³è¿‡æ— æ³•æå–æ—¥æœŸçš„æ–‡ä»¶: {f}")

    # æŒ‰æ—¥æœŸæ’åº
    sorted_dates = sorted(files_map.keys())
    print(f"âœ… æ‰«æåˆ° {len(sorted_dates)} ä¸ªæ—¥æœŸçš„æ–‡ä»¶å¾…å¤„ç†")
    
    # é‡æ–°ç»„è£…æˆæœ‰åºå­—å…¸
    ordered_map = {date: files_map[date] for date in sorted_dates}
    return ordered_map

def read_file_data(file_path):
    """
    è¯»å–å•ä¸ªæ–‡ä»¶ï¼Œè¿”å› {ä»£ç : æŠ˜ç®—ç‡} çš„å­—å…¸
    è‡ªåŠ¨åˆ¤æ–­æ˜¯ä¸Šæµ·è¿˜æ˜¯æ·±åœ³æ ¼å¼ï¼Œå¹¶ç»Ÿä¸€å•ä½ä¸ºæ•´æ•°
    """
    filename = os.path.basename(file_path)
    
    # === 1. åˆ¤æ–­äº¤æ˜“æ‰€æ ¼å¼ ===
    if "æ·±åœ³" in filename:
        header_row = 4
        print(f"   â†’ è¯»å–æ·±åœ³æ–‡ä»¶ (Header=5): {filename}")
    else:
        header_row = 2
        print(f"   â†’ è¯»å–ä¸Šæµ·æ–‡ä»¶ (Header=3): {filename}")

    # === 2. è¯»å–æ–‡ä»¶å†…å®¹ ===
    with open(file_path, "rb") as f:
        file_stream = io.BytesIO(f.read())

    try:
        # å°è¯•è¯» Excel
        df = pd.read_excel(file_stream, header=header_row)
    except:
        # å¤±è´¥åˆ™å°è¯•è¯» CSV (GBKç¼–ç å¸¸è§äºå›½å†…é‡‘èæ•°æ®)
        file_stream.seek(0)
        try:
            df = pd.read_csv(file_stream, header=header_row, sep=None, engine="python", encoding='gbk')
        except:
            df = pd.read_csv(file_stream, header=header_row, sep=None, engine="python", encoding='utf-8')

    # === 3. åŠ¨æ€å¯»æ‰¾åˆ—å ===
    cols = df.columns.tolist()
    # æ¨¡ç³ŠåŒ¹é…ï¼šåˆ—åé‡ŒåŒ…å«'ä»£ç 'çš„ä½œä¸ºkeyï¼ŒåŒ…å«'æŠ˜ç®—'çš„ä½œä¸ºvalue
    col_code = next((c for c in cols if 'ä»£ç ' in str(c)), None)
    col_rate = next((c for c in cols if 'æŠ˜ç®—' in str(c)), None)

    if not col_code or not col_rate:
        # å…œåº•ç­–ç•¥ï¼šæŒ‰ä½ç½®å–
        col_code = cols[0]
        col_rate = cols[2] if len(cols) > 2 else cols[1]

    # === 4. æ¸…æ´—ä¸æ ¼å¼ç»Ÿä¸€ (æ ¸å¿ƒä¿®æ”¹) ===
    df = df.dropna(subset=[col_code])
    
    # è½¬æ¢ä»£ç ä¸ºæ•°å­—
    df[col_code] = pd.to_numeric(df[col_code], errors="coerce")
    df = df.dropna(subset=[col_code])
    df[col_code] = df[col_code].astype("Int64")
    
    # è½¬æ¢æŠ˜ç®—ç‡ä¸ºæ•°å­—
    df[col_rate] = pd.to_numeric(df[col_rate], errors="coerce")
    
    # âš¡ï¸âš¡ï¸âš¡ï¸ æ ¸å¿ƒä¿®æ­£ï¼šæ·±åœ³æ•°æ® x100 âš¡ï¸âš¡ï¸âš¡ï¸
    if "æ·±åœ³" in filename:
        print(f"     âš¡ï¸ æ£€æµ‹åˆ°æ·±åœ³æ•°æ®ï¼Œæ‰§è¡Œ x100 ä¿®æ­£ (ä¾‹: 0.60 -> 60)")
        df[col_rate] = df[col_rate] * 100
    
    # å››èˆäº”å…¥å¹¶è½¬ä¸ºæ•´æ•°
    df[col_rate] = df[col_rate].round(0).astype("Int64")
    
    # ç”Ÿæˆå­—å…¸ {code: rate}
    return dict(zip(df[col_code], df[col_rate]))

def process_date_group(date_str, file_list, df_result):
    """
    å¤„ç†ã€åŒä¸€æ—¥æœŸã€‘çš„æ‰€æœ‰æ–‡ä»¶ï¼Œåˆå¹¶åæ›´æ–°åˆ° df_result
    """
    print(f"ğŸ“… å¼€å§‹å¤„ç†æ—¥æœŸ: {date_str}")
    
    combined_map = {}
    
    # 1. éå†å½“å¤©çš„æ‰€æœ‰æ–‡ä»¶ (ä¸Šæµ· + æ·±åœ³)
    for file_path in file_list:
        try:
            file_map = read_file_data(file_path)
            # æ›´æ–°å¤§å­—å…¸ (åè¯»å–çš„ä¼šè¦†ç›–å…ˆè¯»å–çš„ï¼Œæˆ–è€…äº’è¡¥)
            combined_map.update(file_map)
        except Exception as e:
            print(f"âŒ è¯»å–æ–‡ä»¶å¤±è´¥ {os.path.basename(file_path)}: {e}")

    # 2. åŒ¹é…åˆ°æ€»è¡¨
    # ç¡®ä¿æ€»è¡¨çš„ä»£ç ä¹Ÿæ˜¯æ•°å­—ç±»å‹
    df_result["åŸºé‡‘ä»£ç "] = pd.to_numeric(df_result["åŸºé‡‘ä»£ç "], errors="coerce").astype("Int64")
    
    # æ˜ å°„æ•°æ®
    df_result[date_str] = df_result["åŸºé‡‘ä»£ç "].map(combined_map)
    
    return df_result

def sort_columns(df):
    fixed_cols = ["åŸºé‡‘ä»£ç ", "åŸºé‡‘ç®€ç§°"]
    # æ‰¾å‡ºæ‰€æœ‰æ—¥æœŸåˆ—å¹¶æŒ‰æ—¶é—´æ’åº
    date_cols = sorted([c for c in df.columns if c not in fixed_cols])
    return df[fixed_cols + date_cols]

def send_to_feishu(file_name, summary_text=None):
    """
    å‘é€é£ä¹¦æ¶ˆæ¯
    """
    raw_url = f"https://raw.githubusercontent.com/geniusdingding/bond-etf-auto/main/output/{file_name}"
    
    # æ„é€ é£ä¹¦å¯Œæ–‡æœ¬æ¶ˆæ¯
    data = {
        "msg_type": "post",
        "content": {
            "post": {
                "zh_cn": {
                    "title": "ğŸ“Š ç§‘åˆ›å€ºæŠ˜ç®—ç‡è‡ªåŠ¨æ›´æ–°",
                    "content": [
                        [
                            {"tag": "text", "text": summary_text or "âœ… æ•°æ®å·²æ›´æ–°"}
                        ],
                        [
                            {"tag": "a", "text": "ğŸ“ ç‚¹å‡»ä¸‹è½½æœ€æ–°ç´¯è®¡è¡¨æ ¼", "href": raw_url}
                        ]
                    ]
                }
            }
        }
    }

    headers = {"Content-Type": "application/json"}
    try:
        resp = requests.post(WEBHOOK_URL, data=json.dumps(data), headers=headers)
        print("âœ… é£ä¹¦æ¨é€ç»“æœ:", resp.text)
    except Exception as e:
        print("âŒ é£ä¹¦æ¨é€å¤±è´¥:", e)

if __name__ == "__main__":
    # 1. è¯»å–åå•æ¨¡æ¿
    if not os.path.exists(ETF_PATH):
        # å¦‚æœconfigæ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°è¯•åœ¨æ ¹ç›®å½•æ‰¾
        if os.path.exists("ç§‘åˆ›å€ºåå•.xlsx"):
             ETF_PATH = "ç§‘åˆ›å€ºåå•.xlsx"
        else:
             raise FileNotFoundError(f"âŒ æ‰¾ä¸åˆ°é…ç½®æ–‡ä»¶: {ETF_PATH}ï¼Œè¯·ç¡®ä¿åå•æ–‡ä»¶å­˜åœ¨ã€‚")
    
    df_template = pd.read_excel(ETF_PATH)
    # ä»…ä¿ç•™æ ¸å¿ƒåˆ—ï¼Œé˜²æ­¢æ¨¡æ¿æ±¡æŸ“
    df_template = df_template[["åŸºé‡‘ä»£ç ", "åŸºé‡‘ç®€ç§°"]]

    # 2. åŠ è½½æˆ–æ–°å»ºç»“æœè¡¨
    if os.path.exists(OUTPUT_FILE):
        print(f"âœ… åŠ è½½å†å²æ–‡ä»¶: {OUTPUT_FILE}")
        df_result = pd.read_excel(OUTPUT_FILE)
    else:
        print("âœ… åˆå§‹åŒ–æ–°æ–‡ä»¶")
        df_result = df_template.copy()

    # 3. æŒ‰æ—¥æœŸåˆ†ç»„è·å–æ–‡ä»¶
    grouped_files = group_files_by_date()

    if not grouped_files:
        print("âš ï¸ æ²¡æœ‰éœ€è¦å¤„ç†çš„æ–‡ä»¶ï¼Œè„šæœ¬ç»“æŸ")
        # å³ä½¿æ²¡æœ‰æ–°æ–‡ä»¶ï¼Œä¹Ÿå¯ä»¥é€‰æ‹©ä¸é€€å‡ºï¼Œç»§ç»­æ¨é€æ—§æ•°æ®æ‘˜è¦ï¼Œè¿™é‡Œé€‰æ‹©é€€å‡º
        # exit(0) 

    # 4. å¾ªç¯å¤„ç†æ¯ä¸€å¤©
    for date_str, files in grouped_files.items():
        df_result = process_date_group(date_str, files, df_result)

    # 5. æ•´ç†åˆ—é¡ºåºå¹¶ä¿å­˜
    df_result = sort_columns(df_result)
    
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    df_result.to_excel(OUTPUT_FILE, index=False)
    print(f"ğŸ‰ ç´¯è®¡ç»“æœå·²ä¿å­˜: {OUTPUT_FILE}")

    # === ç”Ÿæˆ Summary å¹¶æ¨é€ ===
    
    # è·å–æœ€æ–°çš„ä¸€åˆ—æ—¥æœŸæ•°æ®
    cols = df_result.columns.tolist()
    date_cols = [c for c in cols if c not in ["åŸºé‡‘ä»£ç ", "åŸºé‡‘ç®€ç§°"]]
    
    if date_cols:
        latest_date = date_cols[-1]
        valid_data = df_result[latest_date].dropna()
        count = len(valid_data)
        avg_rate = round(valid_data.mean(), 2) if count > 0 else 0
        
        summary = (f"ğŸ“… æœ€æ–°æ•°æ®æ—¥æœŸ: {latest_date}\n"
                   f"ğŸ“ˆ å¯è´¨æŠ¼ETFæ•°é‡: {count} åª\n"
                   f"ğŸ’° å¹³å‡æŠ˜ç®—ç‡: {avg_rate}")
        
        print(f"\næ‘˜è¦ä¿¡æ¯:\n{summary}\n")
        send_to_feishu("ç§‘åˆ›å€ºETF_ç´¯è®¡ç»“æœ.xlsx", summary)
    
    # === Git æäº¤ (ä»…åœ¨ GitHub Actions ç¯å¢ƒä¸‹æœ‰æ•ˆ) ===
    # ç®€å•åˆ¤æ–­æ˜¯å¦åœ¨ GitHub Actions ç¯å¢ƒï¼ˆé€šå¸¸ä¼šæœ‰ GITHUB_ACTIONS ç¯å¢ƒå˜é‡ï¼‰
    if os.getenv("GITHUB_ACTIONS"):
        print("ğŸ¤– æ£€æµ‹åˆ° GitHub Actions ç¯å¢ƒï¼Œæ‰§è¡Œ Git æäº¤...")
        os.system(f'git config --local user.email "action@github.com"')
        os.system(f'git config --local user.name "GitHub Action"')
        os.system(f"git add {OUTPUT_FILE}")
        os.system('git commit -m "Auto update bond ETF rates" || echo "No changes to commit"')
        # Push æ­¥éª¤é€šå¸¸åœ¨ yml æ–‡ä»¶é‡Œé…ç½®ï¼Œè¿™é‡Œåª commit
    else:
        print("ğŸ’» æœ¬åœ°è¿è¡Œæ¨¡å¼ï¼Œè·³è¿‡è‡ªåŠ¨ Commitã€‚")