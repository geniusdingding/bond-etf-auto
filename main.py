import os
import io
import re
import pandas as pd
from datetime import datetime
import requests
import json
import collections

# ================= é…ç½®åŒºåŸŸ =================
INPUT_DIR = "input"       # æ”¾å…¥æ¯å¤©ä¸‹è½½çš„ xls æ–‡ä»¶çš„ç›®å½•
OUTPUT_DIR = "output"     # ç»“æœä¿å­˜ç›®å½•
ETF_PATH = os.path.join("config", "ç§‘åˆ›å€ºåå•.xlsx")  # æ‚¨çš„åå•æ¨¡æ¿è·¯å¾„
OUTPUT_FILE = os.path.join(OUTPUT_DIR, "ç§‘åˆ›å€ºETF_ç´¯è®¡ç»“æœ.xlsx")
# è¯·æ›¿æ¢ä¸ºæ‚¨çš„çœŸå®é£ä¹¦ Webhook
WEBHOOK_URL = "https://open.feishu.cn/open-apis/bot/v2/hook/fc7e6de2-fa45-4c14-96ac-c7bda5874732"
# ===========================================

def load_push_config():
    cfg_path = "config.json"
    if not os.path.exists(cfg_path):
        return False
    try:
        with open(cfg_path, "r", encoding="utf-8") as f:
            cfg = json.load(f)
            return cfg.get("push_enabled", False)
    except:
        return False

def extract_date_from_filename(filename: str) -> str:
    """
    ä»æ–‡ä»¶åä¸­æå–8ä½æ•°å­—æ—¥æœŸï¼Œä¾‹å¦‚ 20251124 -> 2025/11/24
    """
    basename = os.path.basename(filename)
    m = re.search(r"(\d{8})", basename)
    if not m:
        return None
    date_str = m.group(1)
    return f"{date_str[0:4]}/{date_str[4:6]}/{date_str[6:8]}"

def group_files_by_date():
    """
    æ‰«æ input ç›®å½•ï¼ŒæŒ‰æ—¥æœŸå°†æ–‡ä»¶åˆ†ç»„
    è¿”å›æ ¼å¼: { '2025/11/26': ['input/ä¸Šæµ·...xls', 'input/æ·±åœ³...xls'], ... }
    """
    files_map = collections.defaultdict(list)
    
    if not os.path.exists(INPUT_DIR):
        os.makedirs(INPUT_DIR)
        print(f"âš ï¸ ç›®å½• {INPUT_DIR} ä¸å­˜åœ¨ï¼Œå·²è‡ªåŠ¨åˆ›å»ºï¼Œè¯·æ”¾å…¥ xls æ–‡ä»¶ã€‚")
        return {}

    raw_files = [f for f in os.listdir(INPUT_DIR) if f.lower().endswith((".xls", ".xlsx", ".csv"))]
    
    if not raw_files:
        print("âš ï¸ input ç›®å½•æ²¡æœ‰ä»»ä½• Excel æ–‡ä»¶")
        return {}

    for f in raw_files:
        date_str = extract_date_from_filename(f)
        if date_str:
            full_path = os.path.join(INPUT_DIR, f)
            files_map[date_str].append(full_path)

    # æŒ‰æ—¥æœŸæ’åº
    sorted_dates = sorted(files_map.keys())
    print(f"âœ… æ‰«æåˆ° {len(sorted_dates)} ä¸ªæ—¥æœŸçš„æ–‡ä»¶å¾…å¤„ç†")
    
    return {date: files_map[date] for date in sorted_dates}

def read_file_data(file_path):
    """
    è¯»å–å•ä¸ªæ–‡ä»¶ï¼Œè¿”å› {ä»£ç : æŠ˜ç®—ç‡} çš„å­—å…¸
    è‡ªåŠ¨åˆ¤æ–­æ˜¯ä¸Šæµ·è¿˜æ˜¯æ·±åœ³æ ¼å¼ï¼Œå¹¶ç»Ÿä¸€å•ä½ä¸ºæ•´æ•°
    """
    filename = os.path.basename(file_path)
    
    # === 1. åˆ¤æ–­äº¤æ˜“æ‰€æ ¼å¼ ===
    if "æ·±åœ³" in filename:
        header_row = 4
        print(f"   â†’ è¯»å–æ·±åœ³æ–‡ä»¶ (Header=5): {filename}")
    else:
        header_row = 2
        print(f"   â†’ è¯»å–ä¸Šæµ·æ–‡ä»¶ (Header=3): {filename}")

    # === 2. è¯»å–æ–‡ä»¶å†…å®¹ ===
    with open(file_path, "rb") as f:
        file_stream = io.BytesIO(f.read())

    try:
        # å°è¯•è¯» Excel
        df = pd.read_excel(file_stream, header=header_row)
    except:
        # å¤±è´¥åˆ™å°è¯•è¯» CSV
        file_stream.seek(0)
        try:
            df = pd.read_csv(file_stream, header=header_row, sep=None, engine="python", encoding='gbk')
        except:
            df = pd.read_csv(file_stream, header=header_row, sep=None, engine="python", encoding='utf-8')

    # === 3. åŠ¨æ€å¯»æ‰¾åˆ—å ===
    cols = df.columns.tolist()
    col_code = next((c for c in cols if 'ä»£ç ' in str(c)), None)
    col_rate = next((c for c in cols if 'æŠ˜ç®—' in str(c)), None)

    if not col_code or not col_rate:
        # å…œåº•ç­–ç•¥
        col_code = cols[0]
        col_rate = cols[2] if len(cols) > 2 else cols[1]

    # === 4. æ¸…æ´—ä¸æ ¼å¼ç»Ÿä¸€ ===
    df = df.dropna(subset=[col_code])
    
    # è½¬æ¢ä»£ç ä¸ºæ•°å­—
    df[col_code] = pd.to_numeric(df[col_code], errors="coerce")
    df = df.dropna(subset=[col_code])
    df[col_code] = df[col_code].astype("Int64")
    
    # è½¬æ¢æŠ˜ç®—ç‡ä¸ºæ•°å­—
    df[col_rate] = pd.to_numeric(df[col_rate], errors="coerce")
    
    # âš¡ï¸âš¡ï¸âš¡ï¸ æ ¸å¿ƒä¿®æ­£ï¼šæ·±åœ³æ•°æ® x100 âš¡ï¸âš¡ï¸âš¡ï¸
    if "æ·±åœ³" in filename:
        print(f"     âš¡ï¸ æ£€æµ‹åˆ°æ·±åœ³æ•°æ®ï¼Œæ‰§è¡Œ x100 ä¿®æ­£")
        df[col_rate] = df[col_rate] * 100
    
    # å››èˆäº”å…¥å¹¶è½¬ä¸ºæ•´æ•°
    df[col_rate] = df[col_rate].round(0).astype("Int64")
    
    return dict(zip(df[col_code], df[col_rate]))

def process_date_group(date_str, file_list, df_result):
    print(f"ğŸ“… å¼€å§‹å¤„ç†æ—¥æœŸ: {date_str}")
    combined_map = {}
    
    for file_path in file_list:
        try:
            file_map = read_file_data(file_path)
            combined_map.update(file_map)
        except Exception as e:
            print(f"âŒ è¯»å–æ–‡ä»¶å¤±è´¥ {os.path.basename(file_path)}: {e}")

    df_result["åŸºé‡‘ä»£ç "] = pd.to_numeric(df_result["åŸºé‡‘ä»£ç "], errors="coerce").astype("Int64")
    df_result[date_str] = df_result["åŸºé‡‘ä»£ç "].map(combined_map)
    return df_result

def sort_columns(df):
    fixed_cols = ["åŸºé‡‘ä»£ç ", "åŸºé‡‘ç®€ç§°"]
    date_cols = sorted([c for c in df.columns if c not in fixed_cols])
    return df[fixed_cols + date_cols]

def send_to_feishu(file_name, summary_text=None, df_preview=None):
    """
    å‘é€é£ä¹¦æ¶ˆæ¯ï¼Œå¢åŠ æ•°æ®é¢„è§ˆ (df_preview)
    """
    raw_url = f"https://raw.githubusercontent.com/geniusdingding/bond-etf-auto/auto-updates/output/{file_name}"
    
    # æ„é€ é¢„è§ˆæ–‡æœ¬ (æŠ½æŸ¥å‡ ä¸ªå…³é”®ETF)
    preview_text = ""
    if df_preview is not None:
        cols = df_preview.columns.tolist()
        date_cols = [c for c in cols if c not in ["åŸºé‡‘ä»£ç ", "åŸºé‡‘ç®€ç§°"]]
        if date_cols:
            latest_date = date_cols[-1]
            # è¿™é‡Œçš„ä»£ç å¯¹åº”: 511120(å¹¿å‘), 159400(å¹³å®‰), 159200(å¯Œå›½)
            target_codes = [511120, 159400, 159200]
            
            # ç­›é€‰
            preview_rows = df_preview[df_preview['åŸºé‡‘ä»£ç '].isin(target_codes)]
            
            if not preview_rows.empty:
                preview_text = "\n\nğŸ” å…³é”®ETFæŠ½æŸ¥ (éªŒè¯æ•´æ•°):\n"
                for _, row in preview_rows.iterrows():
                    val = row[latest_date]
                    val_str = str(val) if pd.notna(val) else "æ— æ•°æ®"
                    preview_text += f"â€¢ {row['åŸºé‡‘ç®€ç§°']}: {val_str}\n"

    # æ„é€ å®Œæ•´æ–‡æ¡ˆ
    full_content = (summary_text or "âœ… æ•°æ®å·²æ›´æ–°") + preview_text
    
    data = {
        "msg_type": "post",
        "content": {
            "post": {
                "zh_cn": {
                    "title": "ğŸ“Š ç§‘åˆ›å€ºæŠ˜ç®—ç‡è‡ªåŠ¨æ›´æ–°",
                    "content": [
                        [{"tag": "text", "text": full_content}],
                        [{"tag": "a", "text": "ğŸ“ ç‚¹å‡»ä¸‹è½½æœ€æ–°ç´¯è®¡è¡¨æ ¼ (GitåŒæ­¥ä¸­)", "href": raw_url}]
                    ]
                }
            }
        }
    }

    try:
        resp = requests.post(WEBHOOK_URL, data=json.dumps(data), headers={"Content-Type": "application/json"})
        print("âœ… é£ä¹¦æ¨é€ç»“æœ:", resp.text)
    except Exception as e:
        print("âŒ é£ä¹¦æ¨é€å¤±è´¥:", e)

if __name__ == "__main__":
    # 1. è¯»å–å¼€å…³é…ç½®
    push_enabled = load_push_config()

    # 2. è¯»å–åå•æ¨¡æ¿
    if not os.path.exists(ETF_PATH):
        if os.path.exists("ç§‘åˆ›å€ºåå•.xlsx"):
             ETF_PATH = "ç§‘åˆ›å€ºåå•.xlsx"
        else:
             raise FileNotFoundError(f"âŒ æ‰¾ä¸åˆ°é…ç½®æ–‡ä»¶: {ETF_PATH}")
    
    df_template = pd.read_excel(ETF_PATH)[["åŸºé‡‘ä»£ç ", "åŸºé‡‘ç®€ç§°"]]

    # 3. åŠ è½½æˆ–æ–°å»ºç»“æœè¡¨
    if os.path.exists(OUTPUT_FILE):
        print(f"âœ… åŠ è½½å†å²æ–‡ä»¶: {OUTPUT_FILE}")
        df_result = pd.read_excel(OUTPUT_FILE)
    else:
        print("âœ… åˆå§‹åŒ–æ–°æ–‡ä»¶")
        df_result = df_template.copy()

    # 4. æ‰«ææ–‡ä»¶å¹¶å¤„ç†
    grouped_files = group_files_by_date()
    
    # å¦‚æœæ²¡æœ‰æ–‡ä»¶ï¼Œè„šæœ¬ç»“æŸï¼Œä¸å‘æ¶ˆæ¯
    if not grouped_files:
        print("âš ï¸ æ²¡æœ‰éœ€è¦å¤„ç†çš„æ–‡ä»¶ï¼Œè„šæœ¬ç»“æŸ")
    else:
        for date_str, files in grouped_files.items():
            df_result = process_date_group(date_str, files, df_result)

        # 5. ä¿å­˜ç»“æœ
        df_result = sort_columns(df_result)
        os.makedirs(OUTPUT_DIR, exist_ok=True)
        df_result.to_excel(OUTPUT_FILE, index=False)
        print(f"ğŸ‰ ç´¯è®¡ç»“æœå·²ä¿å­˜: {OUTPUT_FILE}")

        # 6. ç”Ÿæˆæ‘˜è¦å¹¶æ¨é€ (è¿™é‡Œä¸å†åš Git æäº¤ï¼Œåªè´Ÿè´£é€šçŸ¥)
        cols = df_result.columns.tolist()
        date_cols = [c for c in cols if c not in ["åŸºé‡‘ä»£ç ", "åŸºé‡‘ç®€ç§°"]]
        
        if date_cols:
            latest_date = date_cols[-1]
            valid_data = df_result[latest_date].dropna()
            count = len(valid_data)
            avg_rate = round(valid_data.mean(), 2) if count > 0 else 0
            
            summary = (f"ğŸ“… æ›´æ–°æ—¥æœŸ: {latest_date}\n"
                       f"ğŸ“ˆ å¯è´¨æŠ¼ETFæ•°é‡: {count} åª\n"
                       f"ğŸ’° å¹³å‡æŠ˜ç®—ç‡: {avg_rate}")
            
            print(f"\næ‘˜è¦ä¿¡æ¯:\n{summary}\n")

            if push_enabled:
                # ä¼ å…¥ df_result ä»¥ç”Ÿæˆé¢„è§ˆ
                send_to_feishu("ç§‘åˆ›å€ºETF_ç´¯è®¡ç»“æœ.xlsx", summary, df_result)
                print("ğŸš€ å·²æ‰§è¡Œé£ä¹¦æ¨é€")
            else:
                print("âœ… push_enabled=False â†’ è·³è¿‡é£ä¹¦æ¨é€")

    # 7. Git æ“ä½œå·²å®Œå…¨ç§»é™¤ï¼Œäº¤ç”± YAML æ¥ç®¡